---
title: "CHAPTER 7"
author: "Lam (Serosv)"
format: revealjs
editor: visual
---

## CHAPTER 7: Nonparametric Approaches to Model the Prevalence and Force of Infection {.smaller style="text-align: center;"}

$\space$

![](GeorgeEPBox.jpg){fig-align="center"}

$\space$

. . .

*"All models are wrong, but some are useful"*

***George E. P. Box***

## 7.1 Nonparametric Approaches {.smaller}

. . .

-   In the previous chapter, parametric models were fitted in the framework of GLMs.

. . .

-   Although quite flexible, they are of a predetermined shape through their specific analytical form and might not be able to capture unexpected features of the data.

. . .

-   Nonparametric regression methods allow us to accommodate flexible, nonlinear relationships for the seroprevalence and for the FOI.

. . .

-   There are many nonparametric so-called "smoothing" methods.

##  {.smaller}

-   Nonparametric methods are originating from different approximating principles and are classified into **'series'** and **'non-series'** methods.

. . .

-   Series methods

. . .

-   Non-series methods

. . .

-   The line between the two models can be very thin, so...

. . .

leading to **"semiparametric"** models. These types of models will be discussed later.

$\space$

. . .

**The focus in this chapter in on the LOCAL POLYNOMIAL ESTIMATOR.**

$\space$

. . .

But first...

. . .

We start with **a short historical tour** on the **first nonparametric approaches** in the field of **statistical models for infectious diseases.**

## 7.1.1 The First Nonparametric Approaches {.smaller}

. . .

-   In Chapter 6, **Griffiths** presented his attempt to estimate the FOI using parametric approaches.

. . .

-   He justified his choice for a parametric model with **linear force of infection** using a nonparametric estimate for the same parameter.

. . .

-   Farrington used a smoothed version of Griffiths's estimator. But however, both can **lead to negative** estimate for FOI (postponed).

. . .

This section we will discuss **Keiding's two-step approach:**

. . .

-   Estimate the prevalence by isotonic regression.

. . .

-   Use kernel smoother to estimate the FOI.

##  {.smaller}

-   The isotonic regression estimator of the observed prevalence is the **nonparametric maximum likelihood** **estimator** (NPMLE).

. . .

-   General case of binomial likelihood and in context of infectious diseases.

. . .

-   In the second step the force of infection, assumed to be **a smooth function of age**, is estimated by

. . .

$$
\hat{\lambda}(a) = \frac{1}{h}\int_{a-h}^{a+h}K(\frac{x-a}{h})\frac{d\hat{\pi}(x)}{1- \hat{\pi}(x^-)}
$$

. . .

where $K$ is a kernel function, $h$ is the bandwidth, and $\hat{\pi}()$ is the isotonic regression estimator of the observed prevalence.

##  {.smaller}

In case that $\hat{\pi}$ has discontinuities at $x_1, x_ 2, … x_n$, then

$$
\hat{\lambda}(a) = \frac{1}{h}\sum_{x_i\in [a-h,a+h]}^{}K(\frac{x_i-a}{h})\frac{\hat{\pi}(x_i)-\hat{\pi}(x_{i-1})}{1 -\hat{\pi}(x_{i-1})}
$$

-   Keiding\'s method requires the **crucial choice of an optimal bandwidth** $h$**.**

-   Later on, Keiding et al. proposed to replace the kernel estimate in the second step with a smoothing spline.

-   However, both methods **require** the selection of a **smoothing parameter.**

##  {.smaller style="text-align: center;"}

```{r}
#| echo: false
#| eval: true

hepa1<- list(
AGE = c(0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
	17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,
	35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52,
	53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,
	71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85),
POS = c(4,
	11, 6, 20, 28, 55, 70, 38, 37, 31, 35, 27, 29, 25, 25, 26, 16, 18, 16,
	16, 31, 34, 31, 36, 35, 25, 36, 46, 36, 31, 30, 22, 27, 29, 30, 29, 23,
	17, 20, 20, 20, 22, 26, 18, 13, 18, 21, 21, 18, 20, 18, 13, 12, 8, 7,
	14, 7, 11, 8, 8, 8, 4, 4, 6, 4, 4, 7, 4, 2, 3, 3, 4, 5, 2, 3, 3, 0, 1,
	4, 2, 2, 4, 1, 1, 1, 1),
NEG = c(0, 0, 0, 2, 2, 1, 3, 5, 2, 3, 1, 1, 1,
	2, 2, 1, 2, 5, 4, 0, 4, 10, 11, 9, 9, 8, 10, 19, 8, 12, 10, 20, 15, 20,
	23, 23, 32, 29, 24, 29, 37, 26, 38, 42, 42, 37, 34, 52, 39, 44, 44, 38,
	32, 47, 64, 57, 64, 62, 40, 47, 33, 26, 16, 23, 20, 24, 15, 24, 22, 17,
	19, 20, 7, 18, 12, 10, 15, 12, 9, 14, 10, 12, 9, 6, 8, 5),
NTOT = c(4,
	11, 6, 22, 30, 56, 73, 43, 39, 34, 36, 28, 30, 27, 27, 27, 18, 23, 20,
	16, 35, 44, 42, 45, 44, 33, 46, 65, 44, 43, 40, 42, 42, 49, 53, 52, 55,
	46, 44, 49, 57, 48, 64, 60, 55, 55, 55, 73, 57, 64, 62, 51, 44, 55, 71,
	71, 71, 73, 48, 55, 41, 30, 20, 29, 24, 28, 22, 28, 24, 20, 22, 24, 12,
	20, 15, 13, 15, 13, 13, 16, 12, 16, 10, 7, 9, 6))

hepa1<-data.frame(hepa1$AGE,hepa1$NEG,hepa1$POS,hepa1$POS+hepa1$NEG)
names(hepa1)<-c("AGE","POS","NEG","NTOT")

## Some functions
###################

pavit<- function(datai)
{
	pai1 <- pai2 <- datai
	N <- length(pai1)
	for(i in 1:(N - 1)) {
		if(pai2[i] > pai2[i + 1]) {
			pool <- (pai1[i] + pai1[i + 1])/2
			pai2[i:(i + 1)] <- pool
			k <- i + 1
			for(j in (k - 1):1) {
				if(pai2[j] > pai2[k]) {
				  pool.2 <- sum(pai1[j:k])/length(pai1[j:k])
				  pai2[j:k] <- pool.2
				}
			}
		}
	}
	return(list(pai1, pai2))
}



pavit.w<- function(datai2)
{
	gi<- datai2$POS/datai2$NTOT
	pai1 <- pai2 <- gi
	N <- length(pai1)
	ni<-datai2$NTOT
	for(i in 1:(N - 1)) {
		if(pai2[i] > pai2[i + 1]) {
			pool <- (ni[i]*pai1[i] + ni[i+1]*pai1[i + 1])/(ni[i]+ni[i+1])
			pai2[i:(i + 1)] <- pool
			k <- i + 1
			for(j in (k - 1):1) {
				if(pai2[j] > pai2[k]) {
				  pool.2 <- sum(ni[j:k]*pai1[j:k])/(sum(ni[j:k]))
				  pai2[j:k] <- pool.2
				}
			}
		}
	}
	return(list(pai1, pai2))
}


pava <- function (x, wt=rep(1,length(x)))
#  Compute the isotonic regression of numeric vector 'x', with
#  weights 'wt', with respect to simple order.  The pool-adjacent-
#  violators algorithm is used.  Returns a vector of the same length
#  as 'x' containing the regression.
#  02 Sep 1994 / R.F. Raubertas
{
   n <- length(x)
   if (n <= 1) return (x)
   if (any(is.na(x)) || any(is.na(wt))) {
      stop ("Missing values in 'x' or 'wt' not allowed")
   }
   lvlsets <- (1:n)
   repeat {
      viol <- (as.vector(diff(x)) < 0)  # Find adjacent violators
      if (!(any(viol))) break
      i <- min( (1:(n-1))[viol])        # Pool first pair of violators
      lvl1 <- lvlsets[i]
      lvl2 <- lvlsets[i+1]
      ilvl <- (lvlsets == lvl1 | lvlsets == lvl2)
      x[ilvl] <- sum(x[ilvl]*wt[ilvl]) / sum(wt[ilvl])
      lvlsets[ilvl] <- lvl1
   }
  return(x)
}


## Isotonic regression hepatitis A in BE ##
###########################################

xi<-hepa1$AGE
wi<-hepa1$NTOT
yi<-hepa1$POS/hepa1$NTOT


#### pavit (weighted and unweighted)

iso1<-pava(yi,wt=wi)  # weighted pava
ir <- isoreg(yi~xi)   # unweighted isoreg

j<- hepa1$AGE
gi<- hepa1$POS/hepa1$NTOT
pai<- gi
xx<- pavit.w(hepa1)
xx1<- pavit(pai)

####  FIGURE 7.1
par(las=1,cex.axis=1.1,cex.lab=1.1,lwd=3,mgp=c(2, 0.5, 0),mar=c(4.1,4.1,4.1,3))

plot(hepa1$AGE,gi,cex=0.05*wi,ylim=c(0,1),xlab="age",ylab="seroprevalnce",lwd=1)
#lines(hepa1$AGE,xx[[2]],type="s")
lines(hepa1$AGE,xx1[[2]],type="s",lty=1)
#lines(xi,iso1,type="s")

ageii<-hepa1$AGE
haz1<- c(0,diff(xx1[[2]]))/(1-xx1[[2]])
fit.haz<- ksmooth(ageii,haz1,ker="normal",bandwidth=15,n.points=86)
fit.haz2<- ksmooth(ageii,haz1,ker="normal",bandwidth=20,n.points=86)
lines(fit.haz$x,fit.haz$y*4,lty=1) # kernel
lines(fit.haz2$x,fit.haz2$y*4,lty=2) # kernel
axis(side=4,at=c(0.0,0.2,0.4),labels=c(0.0,0.2,0.4)/4)
mtext(side=4,"force of infection", las=3,line=1.6)
```

**Fig. 7.1** Estimated prevalence and force of infection for Hepatitis A in Belgium using Keiding\'s method: isotonic regression to estimate the prevalence and a kernel smoother for the force of infection, based on a standard normal density K and bandwidths h = 15 (solid curve) and h = 20 (dashed curve).

##  {.smaller}

-   The estimation of the prevalence and the force of infection
    from a serological sample is closely related to the problem of estimation from **current status data.**

. . .

-   While in the literature related to the estimation of parameters from current status data attention is placed on estimating the prevalence, in the context of infectious diseases one focuses on the estimation of the force of infection.

. . .

-   Shiboski proposed **a semiparametric model**, based on generalized additive models, in which the dependency of **the force of infection** on age is modeled **nonparametrically** and **the covariate effect** is the **parametric** component of the model.

. . .

-   Depending on the link function, the model proposed by Shiboski (1998) assumes proportionality; proportional hazards (complementary log--log link) or proportional odds (logit and probit links).

. . .

-   The use of a kernel function to smooth out neighboring data information will be discussed in next section.

##  {.smaller style="text-align: center;"}

```{r}
#| echo: false
#| eval: true
rm(list=ls(all=TRUE))

# Some kernel functions
tricube=function(u){
(1-(abs(u))^3)^3
}

tricubes=function(u){
70/81*(1-(abs(u))^3)^3*(abs(u)<=1)
}

epans=function(u){
3/4*(1-u^2)*(abs(u)<=1)
}

gausss=function(u){
dnorm(u)
}

### FIGURE 7.2
par(las=1,cex.axis=1.1,cex.lab=1.1,lwd=3,mgp=c(2, 0.5, 0),mar=c(4.1,4.1,4.1,3))
grid=seq(-1,1,by=0.01)
plot(grid,3*gausss(3*grid),type="n",xlab="u",ylab="kernel function")
lines(grid,3*gausss(3*grid))
lines(grid,epans(grid),lty=2)
lines(grid,tricubes(grid),lty=3)
```

**Fig. 7.2** Kernel functions: tricube (dotted line), Epanechnikov (dashed line), and Gaussian (solid line)

## 7.1.2 Local Estimation by Polynomials {.smaller}

. . .

-   Within the local polynomial framework, **the linear predictor** $\eta(a)$ is approximated locally, at one particular value $a_0$ for age, **by a line** (local linear, degree $p =$ 1), or **a parabola** (local quadratic, degree $p =$ 2), etc.

. . .

-    The appropriate degree p is a first choice to be made. For a general degree

. . .

$$
\eta(a_i) \approx \eta(a_0) + \eta^{(1)}(a_0)(a_i - a_0)+\frac{\eta^{(2)}(a_0)}{2}(a_i - a_0)^2 +…+ \frac{\eta^{(p)}(a_0)}{p!}(a_i - a_0)^p
$$

. . .

under the condition that $a_i$ is close enough to $a_0$, and that the curve $\eta()$ is sufficiently smooth at the age $a_0$ (smoothness is mathematically translated into the existence of the derivatives $\eta^{(1)}(a_0),…,\eta^{(p)}(a_0)$).

## {.smaller}

-   This **\"closeness\"** is governed by the so-called **kernel function** $K_h$, assigning high weights to data points with age values close to $a_0$ and low or zero weights to data points further or far away.

. . .

-    The kernel function $K_h$ is typically a density function (such as the bell-shaped gaussian density), having mean 0 and variance $h$.

. . .

-   This variance parameter $h$ is the so-called **smoothing parameter.**

. . .

-   Figure 7.2 graphs three popular kernels:

    The Gaussian kernel

    The tricube kernel $(70/81 * (1 - (\mid u \mid)^3)^3 (\mid u \mid)\leq 1)$.

    The Epanechnikov kernel $(3/4 * (1 - u^2) (\mid u \mid \leq 1))$.

. . .

-   This idea to **use a variance parameter to control and optimize the level of smoothness** is a recurrent concept in smoothing.

## {.smaller}

In general, one can say that a **degree** $p$ **higher than two** **is seldom required**, that the choice of the form of the kernel $K_h$ is relatively unimportant compared to **the most crucial parameter** in the game, **the smoothing parameter** $h$, also called the \"window width.\"

. . .

-    There are several options to select an appropriate value for $h$ (will be discused in Section 7.2).

. . .

Instead of maximizing, the local polynomial approach is based on the
maximization of

. . .

$$
\sum_{i = 1}^{N} l_i \left \{Y_i, g^{-1}(\beta_0 + \beta_1(a_i - a_0)+\beta_2(a_i - a_0)^2)+…+B_p(a_i - a_0)^p)  \right \}K_h(a_i - a_0)
$$

. . .

where $l_i \left \{ Y_i, \pi \right \} = Y_ilog\left \{ \pi \right \} + (1-Y_i) log \left \{ 1 - \pi \right \}$ are the individual binomial type contributions to the likelihood.

##  {.smaller}

Identification of the polynomial expressions leads to the following estimator for the $k$-th derivative of $\eta(a_0)$, for $k$ = 0,1,\..., $p$:

$$
\hat{\eta}^{(k)}(a_0) = k!\hat{\beta}_k(a_0)
$$

. . .

**The estimator for the seroprevalence** at age a0 is then given by

$$
\hat{\pi}(a_0) = g^{-1} \left \{ \hat{\beta}_0(a_0)  \right \}
$$

. . .

and for **the force of infection** at age $a_0$ by assuming $p \geq 1$ and using identity:

$$
\hat{\lambda}(a_0) = \hat{\beta}_1(a_0) \delta \left\{ \hat{\beta}_0 (a_0) \right\}
$$

. . .

where $\delta \left\{ \hat{\beta}_0 (a_0) \right\} = dg^{-1}\left\{ \hat{\beta}_0 (a_0) \right\}/d\hat{\beta}_0(a_0)$.

## {.smaller}

Indeed, as likelihood expression indicates, the local fit at a particular value $a_0$ is the result of a simple weighted logistic regression with weights $w(a) = K_h(a − a_0)$.

. . .

-   Figure 7.3 shows two such local fits at the age $a_0$ = 5.5 and $a_0$ = 20.5 for the UK Mumps data. It concerns a local quadratic fit with tricube kernel.

. . .

```{r}
#| echo: false
#| eval: true


library(locfit)

### Reading the parvo data
data<-read.table("C:\\R\\serosv\\data\\MUMPSUK.dat",header=T)
attach(data)
y=pos/ntot
a=age

### FIGURE 7.3
par(las=1,cex.axis=1.1,cex.lab=1.1,lwd=3,mgp=c(2, 0.5, 0),mar=c(4.1,4.1,4.1,3))
plot(age,pos/ntot,cex=0.005*ntot,pch=21,xlab="age",ylab="seroprevalence",ylim=c(0,1))
lpfit<-locfit(y~a,family="binomial")
lpfitd1<-locfit(y~a,deriv="a",family="binomial")
lpfitd2<-locfit(y~a,deriv=c("a","a"),family="binomial")
#cbind(age,fitted(lpfit),fitted(lpfitd1),fitted(lpfitd2))
lines(a,fitted(lpfit),lty=1,lwd=2)

ageg=seq(from=min(age),to=max(age),by=0.1)
a0=5
a0v=5.5
b0=fitted(lpfit)[a0]
b1=fitted(lpfit)[a0]*(1-fitted(lpfit)[a0])*fitted(lpfitd1)[a0]
b2=fitted(lpfit)[a0]*(1-fitted(lpfit)[a0])*((1-2*fitted(lpfit)[a0])*(fitted(lpfitd1)[a0])^2+fitted(lpfitd2)[a0])/2
locpol2ata0=b0+b1*(ageg-a0v)+b2*(ageg-a0v)^2
lines(ageg,locpol2ata0,lwd=2)
points(a0v,fitted(lpfit)[a0],cex=1,pch=24,lwd=1)
points(a0v,fitted(lpfit)[a0],cex=2,pch=24)
abline(v=a0v,lty=2,lwd=1)
h=quantile(abs(age-a0v),probs=0.7)
kernf=tricube((ageg-a0v)/h)/10
lines(ageg[kernf>=0],kernf[kernf>=0],lty=3)


a0=20
a0v=20.5
b0=fitted(lpfit)[a0]
b1=fitted(lpfit)[a0]*(1-fitted(lpfit)[a0])*fitted(lpfitd1)[a0]
b2=fitted(lpfit)[a0]*(1-fitted(lpfit)[a0])*((1-2*fitted(lpfit)[a0])*(fitted(lpfitd1)[a0])^2+fitted(lpfitd2)[a0])/2
locpol2ata0=b0+b1*(ageg-a0v)+b2*(ageg-a0v)^2
lines(ageg,locpol2ata0,lwd=2)
points(a0v,fitted(lpfit)[a0],cex=1,pch=24,lwd=1)
points(a0v,fitted(lpfit)[a0],cex=2,pch=24)
abline(v=a0v,lty=2,lwd=1)
h=quantile(abs(age-a0v),probs=0.7)
kernf=tricube((ageg-a0v)/h)/10
lines(ageg[kernf>=0],kernf[kernf>=0],lty=3)
```

# Thank you for listening! {style="text-align: center;"}
